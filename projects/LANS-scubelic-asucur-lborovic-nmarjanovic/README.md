# LLM Answer Watcher

<div align="center">

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)

**Monitor how Large Language Models talk about your brand vs. competitors.**

</div>

## Overview

LLM Answer Watcher is a comprehensive tool designed to monitor, analyze, and track brand mentions within responses generated by Large Language Models (LLMs). It helps you understand your market positioning in the AI era by comparing your brand's visibility against competitors in buyer-intent queries.

It features a powerful CLI for execution and analysis, a modern Web UI for visualization, and a robust API for integration.

## Key Features

### üîç Brand Monitoring
- **Mention Detection:** Accurately identifies brand mentions using advanced regex matching.
- **Rank Extraction:** Automatically detects your brand's position in ranked lists (e.g., "Top 10 Tools").
- **Sentiment Analysis:** Analyzes the tone and context of mentions.
- **Competitor Tracking:** Side-by-side comparison with competitor brands.

### ü§ñ Multi-Provider Support
- **Standard APIs:** Google Gemini, Groq, OpenAI, Anthropic, Mistral, Perplexity.
- **Browser Runners (New!):** Interact with web-based LLM interfaces (ChatGPT, Perplexity) using headless browser automation (via Steel SDK). Captures screenshots and HTML snapshots of the actual user experience.

### üõ†Ô∏è Powerful CLI Tools
- **`run`:** Execute queries across multiple models in parallel (async/await).
- **`demo`:** Try the tool immediately with mock data‚Äîno API keys required.
- **`eval`:** Built-in quality assurance framework to test extraction accuracy against ground-truth fixtures.
- **`prices`:** Manage and view real-time LLM pricing (integrated with llm-prices.com).
- **`costs`:** Analyze historical spending by provider, model, and timeframe.
- **`validate`:** Verify your configuration before execution.

### üß† Intelligent API
- **Prompt Optimization:** Includes a "Professional Prompt Engineer" endpoint that rewrites simple user queries into high-quality, analytical prompts for better LLM evaluation.
- **REST API:** FastAPI-based backend for running jobs, retrieving results, and managing configuration.

### üìä Visualization & Reporting
- **Web UI:** Modern React/Vite dashboard for interactive data exploration.
- **HTML Reports:** Auto-generated, shareable reports for every run.
- **Data Export:** Export mentions and run summaries to CSV or JSON.
- **SQLite Storage:** All data is stored locally for long-term trend analysis.

## Getting Started

### Option 1: Docker (Recommended)

The simplest way to get started is by using Docker Compose, which sets up the API, Web UI, and database.

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/nibzard/llm-answer-watcher.git
    cd llm-answer-watcher
    ```

2.  **Configure Environment:**
    ```bash
    cp .env.example .env
    # Edit .env with your API keys (e.g., GEMINI_API_KEY, OPENAI_API_KEY)
    ```

3.  **Start the application:**
    ```bash
    docker-compose up -d --build
    ```

4.  **Access:**
    *   **Web UI:** `http://localhost:3000`
    *   **API:** `http://localhost:8000`

### Option 2: CLI Usage (Python)

Run the tool directly from your terminal.

1.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **Try the Demo (No keys needed):**
    ```bash
    llm-answer-watcher demo
    ```

3.  **Create Configuration:**
    ```bash
    cp examples/default.config.yaml my-config.yaml
    # Edit my-config.yaml to define your brands and intents
    ```

4.  **Run the Watcher:**
    ```bash
    export GEMINI_API_KEY=your_key_here
    llm-answer-watcher run --config my-config.yaml
    ```

## CLI Commands Reference

| Command | Description |
|---------|-------------|
| `run` | Execute LLM queries and generate reports. |
| `demo` | Run interactive demo with sample data (no API keys needed). |
| `eval` | Run evaluation suite to test extraction accuracy. |
| `validate` | Validate configuration file format and API keys. |
| `prices` | Show, refresh, or list LLM pricing data. |
| `costs` | Show historical cost breakdown by period/model. |
| `export` | Export mentions (`export mentions`) or runs (`export runs`) to CSV/JSON. |

## Configuration Example

`my-config.yaml`:
```yaml
run_settings:
  output_dir: "./output"
  sqlite_db_path: "./output/watcher.db"
  models:
    - provider: "google"
      model_name: "gemini-1.5-flash"
      env_api_key: "GEMINI_API_KEY"

brands:
  mine:
    - "MyBrand"
    - "MyProduct"
  competitors:
    - "CompetitorA"
    - "CompetitorB"

intents:
  - id: "product-comparison"
    prompt: "What are the best tools for [category]?"
```

## Project Structure

```
.
‚îú‚îÄ‚îÄ config/                  # Configuration templates
‚îú‚îÄ‚îÄ docs/                    # Comprehensive MkDocs documentation
‚îú‚îÄ‚îÄ examples/                # Example configurations (API, Browser, Web Search)
‚îú‚îÄ‚îÄ llm_answer_watcher/      # Python Package
‚îÇ   ‚îú‚îÄ‚îÄ api.py               # FastAPI backend
‚îÇ   ‚îú‚îÄ‚îÄ cli.py               # CLI entry point
‚îÇ   ‚îú‚îÄ‚îÄ evals/               # Evaluation framework
‚îÇ   ‚îú‚îÄ‚îÄ extractor/           # Brand mention & rank extraction logic
‚îÇ   ‚îú‚îÄ‚îÄ llm_runner/          # LLM integrations (API & Browser)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ browser/         # Steel SDK browser automation
‚îÇ   ‚îú‚îÄ‚îÄ report/              # HTML report generator
‚îÇ   ‚îî‚îÄ‚îÄ storage/             # SQLite database & Data Access Object
‚îú‚îÄ‚îÄ web-ui/                  # React/TypeScript Frontend
‚îú‚îÄ‚îÄ tests/                   # Pytest suite
‚îî‚îÄ‚îÄ requirements.txt         # Python dependencies
```

## Documentation

Full documentation is available in the `docs/` directory.

- **[Browser Runners Implementation](docs/BROWSER_RUNNERS.md)**: Details on the new Steel SDK integration.
- **[New Model Compatibility](docs/NEW_MODEL_COMPATIBILITY.md)**: Guide for adding new LLM providers.
- **[Browser Runner Summary](BROWSER_RUNNER_IMPLEMENTATION_SUMMARY.md)**: Implementation status of browser automation.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
