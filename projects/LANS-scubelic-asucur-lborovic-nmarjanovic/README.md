# LLM Answer Watcher

## Overview

LLM Answer Watcher is a powerful tool designed to monitor and analyze brand mentions within the responses generated by Large Language Models (LLMs). It provides insights into how your brands, products, and services are positioned relative to competitors in LLM outputs, helping you understand market perception and identify opportunities.

This project enhances an existing Command-Line Interface (CLI) tool with a user-friendly Graphical User Interface (GUI) for easier data visualization, analysis, and interactive exploration of LLM responses.

## Features

*   **Brand Monitoring:** Track mentions of your brands and competitors in LLM generated text.
*   **Customizable Intents:** Define specific queries or prompts to evaluate LLM responses.
*   **Multi-Model Support:** Integrate with various LLM providers (e.g., Google Gemini).
*   **Data Visualization:** Interactive Web UI for analyzing trends and patterns in brand mentions.
*   **Historical Data:** Stores results in a SQLite database for long-term tracking and analysis.
*   **Reporting:** Generates detailed HTML reports for each analysis run.

## Technologies Used

*   **Backend:** Python 3.9+ (FastAPI, Pydantic, SQLAlchemy)
*   **Frontend:** React, TypeScript, Vite, TailwindCSS
*   **Database:** SQLite
*   **Containerization:** Docker, Docker Compose
*   **LLM Providers:** Google Gemini, Groq (extensible to others)

## Prerequisites

Before you begin, ensure you have the following installed:

*   **Git:** For cloning the repository.
*   **Docker & Docker Compose (Recommended):** For the easiest setup and deployment.
*   **Python 3.9+ (Optional, for manual setup):**
    *   `pip` (Python package installer)
    *   `venv` (Python virtual environment)
*   **Node.js & npm (Optional, for manual Web UI setup):**
    *   Node.js 18+
    *   npm 9+

## Getting Started

### Option 1: Docker (Recommended)

The simplest way to get started is by using Docker Compose, which automatically sets up both the API server and the web interface.

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/LLM-Answer-Watcher.git
    cd LLM-Answer-Watcher
    ```

2.  **Create an `.env` file:**
    Copy the example environment file and fill in your LLM API keys.
    ```bash
    cp .env.example .env
    ```
    Edit `.env` and replace `YOUR_GEMINI_API_KEY` with your actual API key.

3.  **Start the application:**
    ```bash
    docker-compose up --build
    ```
    To run in the background:
    ```bash
    docker-compose up -d --build
    ```

4.  **Access the application:**
    *   **Web UI:** `http://localhost:3000`
    *   **API:** `http://localhost:8000`

5.  **Stop the application:**
    ```bash
    docker-compose down
    ```

### Option 2: Manual Installation (Backend & Frontend)

This option provides more control but requires separate setup for the backend and frontend.

#### 1. Backend Setup

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/LLM-Answer-Watcher.git
    cd LLM-Answer-Watcher
    ```

2.  **Create and activate a Python virtual environment:**
    ```bash
    python -m venv .venv
    # On Windows:
    .venv\Scripts\activate
    # On macOS/Linux:
    source .venv/bin/activate
    ```

3.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set up environment variables:**
    Copy `.env.example` to `.env` and populate your LLM API keys.
    ```bash
    cp .env.example .env
    ```
    Load these variables into your environment (e.g., `export GEMINI_API_KEY=AIza...` or use a tool like `python-dotenv`).

5.  **Run the backend API:**
    ```bash
    uvicorn llm_answer_watcher.api:app --host 0.0.0.0 --port 8000
    ```

#### 2. Frontend Setup

1.  **Navigate to the `web-ui` directory:**
    ```bash
    cd web-ui
    ```

2.  **Install Node.js dependencies:**
    ```bash
    npm install
    ```

3.  **Start the development server:**
    ```bash
    npm run dev
    ```
    The Web UI will typically be available at `http://localhost:5173` (or another port indicated by Vite).

## Configuration

Before running the watcher, you need to create and configure an analysis file.

1.  **Copy the example configuration:**
    ```bash
    cp examples/default.config.yaml my-config.yaml
    ```

2.  **Edit `my-config.yaml`:**
    Customize this file according to your needs. Key sections to configure include:

    *   **`run_settings`**:
        *   `output_dir`: Directory where results will be stored (e.g., `./output`).
        *   `sqlite_db_path`: Path to the SQLite database for historical data (e.g., `./output/watcher.db`).
        *   `models`: A list of LLM models to use. Each model requires a `provider` (e.g., `google`), `model_name` (e.g., `gemini-1.5-flash`), and `env_api_key` (the name of the environment variable containing your API key).

    *   **`brands`**:
        *   `mine`: A list of your brands and their aliases.
        *   `competitors`: A list of competitor brands.

    *   **`intents`**: A list of queries you want to monitor. Each intent has an `id` and a `prompt` (the query text).

    Example `my-config.yaml`:
    ```yaml
    run_settings:
      output_dir: "./output"
      sqlite_db_path: "./output/watcher.db"
      max_concurrent_requests: 10
      models:
        - provider: "google"
          model_name: "gemini-1.5-flash"
          env_api_key: "GEMINI_API_KEY"
          system_prompt: "google/default"

    brands:
      mine:
        - "MyBrand"
        - "MyProduct"
      competitors:
        - "CompetitorA"
        - "CompetitorB"

    intents:
      - id: "product-comparison"
        prompt: "What are the best tools for [category]?"
      - id: "purchase-decision"
        prompt: "Which [product type] should I choose?"
    ```

    Ensure your API keys are set as environment variables before running (e.g., `export GEMINI_API_KEY=AIza...`).

## Usage

After configuring `my-config.yaml`, you can run the LLM Answer Watcher:

```bash
llm-answer-watcher run --config my-config.yaml
```

The tool will execute queries against the defined LLM models, analyze their responses for brand mentions, and store the results.

### Exploring Results

Results are saved in the directory specified by `output_dir` (e.g., `./output`). Within this directory, you will find:

*   **`watcher.db`**: An SQLite database containing all collected data.
*   **HTML Reports**: An HTML file with a detailed report for each `run`.
*   **Web UI**: If you are running the Docker setup or have manually started the frontend, you can explore and analyze results through the interactive web interface.

## Project Structure

```
.
├── .github/                 # GitHub Actions workflows
├── .claude/                 # Claude agent configurations (if applicable)
├── config/                  # Configuration files
├── docs/                    # Project documentation (MkDocs)
├── examples/                # Example configuration files
├── llm_answer_watcher/      # Python backend application
│   ├── api.py               # FastAPI application entry point
│   ├── cli.py               # Command-Line Interface definitions
│   ├── config/              # Backend configuration logic
│   ├── llm_runner/          # LLM client implementations
│   ├── report/              # Report generation logic
│   ├── storage/             # Database interaction (SQLite)
│   └── ...
├── web-ui/                  # React/TypeScript frontend application
│   ├── src/                 # Frontend source code
│   ├── public/              # Static assets
│   ├── package.json         # Node.js dependencies
│   └── ...
├── tests/                   # Unit and integration tests
├── .env.example             # Example environment variables
├── Dockerfile               # Docker image definition
├── docker-compose.yml       # Docker Compose setup
├── requirements.txt         # Python dependencies
├── pyproject.toml           # Python project configuration (e.g., Ruff)
└── README.md                # This file
```

## Contributing

We welcome contributions to the LLM Answer Watcher! Please see our [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on how to contribute, report bugs, and suggest features.

## License

This project is licensed under the [LICENSE](LICENSE) file.